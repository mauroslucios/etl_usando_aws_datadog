# ETL Motivo de Decis√£o

Este projeto implementa um processo **ETL (Extract, Transform, Load)** utilizando **PySpark**, **MySQL**, **MongoDB** e **LocalStack (S3)** para simular a integra√ß√£o de dados e armazenamento de resultados de an√°lises de cr√©dito.

## üìå Objetivo

O pipeline executa as seguintes etapas:
1. **Extra√ß√£o** de dados de tabelas relacionais (MySQL).
2. **Transforma√ß√£o** aplicando regras de neg√≥cio:
   - Cliente deve morar em zona **rural**;
   - Tipo de pessoa: **PF**;
   - Soma dos bens (`valor_total_bens`) **>= 300.000**;
   - N√£o estar negativado.
3. **Carga** dos resultados:
   - Arquivos **JSON particionados** em um bucket S3 (via LocalStack).
   - Opcionalmente, grava em **MySQL** e **MongoDB**.

---

## üöÄ Tecnologias Utilizadas

- [Python 3.11](https://www.python.org/)
- [PySpark](https://spark.apache.org/)
- [MySQL](https://www.mysql.com/)
- [MongoDB](https://www.mongodb.com/)
- [LocalStack](https://localstack.cloud/)
- [Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) (SDK AWS em Python)

---

## üìÇ Estrutura do Projeto

```
.
‚îú‚îÄ‚îÄ etl_motivo_decisao.py   # Script principal (orquestra√ß√£o do ETL)
‚îú‚îÄ‚îÄ ETL.py                  # Classe ETL (extra√ß√£o, transforma√ß√£o, carga)
‚îú‚îÄ‚îÄ ETLLogger.py            # Classe para logs estruturados
‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îî‚îÄ‚îÄ etl.log             # Arquivo de logs gerados
‚îú‚îÄ‚îÄ jars/
‚îÇ   ‚îî‚îÄ‚îÄ mysql-connector-j-8.0.33.jar
‚îî‚îÄ‚îÄ README.md
```

---

## ‚öôÔ∏è Pr√©-requisitos

- Docker e Docker Compose
- LocalStack rodando localmente
- MySQL e MongoDB ativos (em containers ou servi√ßos externos)
- Vari√°veis de ambiente configuradas (padr√£o j√° incluso no c√≥digo):
  ```bash
  MYSQL_URL=jdbc:mysql://172.22.1.2:3306/elysium
  MYSQL_USER=root
  MYSQL_PASSWORD=123456
  MONGO_URI=mongodb://172.15.0.3:27017/elysium_db
  LOCALSTACK_ENDPOINT=http://localhost:4566
  S3_BUCKET=motivo-de-decisao
  S3_PREFIX=analises/
  ```

---

## ‚ñ∂Ô∏è Como Executar

1. **Subir o LocalStack:**
   ```bash
   docker-compose up -d localstack
   ```

2. **Executar o ETL:**
   ```bash
   export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
   export PATH=$JAVA_HOME/bin:$PATH
   spark-submit --jars mysql-connector-j-8.0.33.jar main.py
   ```

3. **Verificar logs:**
   ```bash
   tail -f logs/etl.log
   ```

---

## üì¶ Exemplos de Uso (S3 via LocalStack)

### Listar arquivos gerados no bucket:
```bash
awslocal s3 ls s3://motivo-de-decisao/analises/
```

### Baixar um arquivo de an√°lise:
```bash
awslocal s3 cp s3://motivo-de-decisao/analises/part-00000.json .
```

### Visualizar o conte√∫do:
```bash
cat part-00000.json | jq .
```

---

## üìù Estrutura de Log

Cada a√ß√£o do ETL √© registrada no arquivo `logs/etl.log` com:
- **timestamp**
- **evento**
- **banco/tabela**
- **a√ß√£o**
- **resultado**
- **erro (se houver)**

Exemplo:
```
2025-09-27 22:15:03,245 - INFO - [Extra√ß√£o de tabela] Database: elysium Table: cliente Action: Lendo dados Result: Sucesso
2025-09-27 22:15:07,892 - ERROR - [Carga no S3] Database: motivo-de-decisao Table: analises Action: Upload Result: Erro Error: ...
```

---

## üìå Pr√≥ximos Passos

- Adicionar testes unit√°rios (PyTest).
- Melhorar performance da escrita em S3 (uso de `coalesce` ou `repartition`).
- Criar docker-compose para orquestrar Spark + MySQL + Mongo + LocalStack.

![LocalStack](https://github.com/mauroslucios/etl_usando_aws_datadog/issues/5#issue-3463030524)